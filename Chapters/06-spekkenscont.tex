\chapter[A revised notion of non-classicality \\ Spekkens contextuality]{A revised notion of non-classicality \\ \huge{Spekkens contextuality}}
\lhead{\emph{Spekkens contextuality}}
\label{sec:spekkcont}
\section{Outcome determinism for unsharp measurements (ODUM)}
\label{sec:odum}
We want a notion of classicality that accomodates unsharp measurements and is robust to noise. A first approach might entail simply extending KS non-contextuality to general POVM measurements, in the sense that a classical HVM description assigns a conditional probability  $p\in\{0,1\}$ to all positive semi-definite operators, such that for every resolution of the identity only one operator receives the valuation $1$. This incremental approach has been given the name ``ODUM", which stands for ``outcome determinism for unsharp measurements". The problem with ODUM is that it runs into numerous inconsistencies, as convincingly argued in \cite{Spekkens2014}. We confine ourselves to the most immediate inconsistency: Consider the quantum experiment consisting of an experimenter repeatedly performing the trivial POVM $\{\frac{\mathbb{1}}{2},\frac{\mathbb{1}}{2}\}$ on some arbitrary preparation $\rho$. Any classical value assignment to positive semi-definite operators in accordance with ODUM is in conflict with the normalization conditions for general ontological models in Section \ref{sec:hvm}. In particular, for a given ontic state $\lambda$, either the added probabilities of the two outcomes is zero or exceeds one. Thus, either this trivial POVM is non-classical according to the extended notion just presented, or the new notion is not meaningful. The trivial POVM corresponds to a fair coin flip - it can be implemented by simply throwing away the system and generating a random bit. Such a ``measurement" should by all means be considered classical. For this implementation, the only sensible choice is to assign to every every ontic state $\lambda$ the conditional probability $\frac{1}{2}$, as measurement and system are completely decoupled. The same line of reasoning leads to the conclusion that the approach in Section \ref{sec:cswhierarch} cannot be extended to general unsharp measurements, as this produces inconsistencies when treating QM.

\section{Operational approach due to Spekkens}
\label{sec:spekkensopappr}
The previous examples highlight the need for a new notion of contextuality that captures the spirit of KS non-contextuality, but is applicable to realistic experimental scenarios. Let us take a step back and identify common features of the contextuality scenarios discussed so far, which led to behaviour we deemed non-classical. The setting was in all cases for the most part identical: we considered sets of projective measurements, with the key property that a given projector was shared amongst multiple measurements. Identical projectors appearing in multiple measurement contexts were crucial for proving a contradiction. Spekkens' approach captures this key feature and operationalizes the underlying assumptions. In QM, a positive semi-definite operator defines an operational equivalence class, in the sense that measurement events which are operationally indistinguishable for all state preparations must be represented by the same operator. An analogous observation applies to density operators describing the preparation of a system.
On that account, instead of assuming a projector to be part of several measurement contexts, Spekkens assumes certain measurement events to be operationally equivalent; we will formalize this shortly. Assuming KS non-contextuality, the implication of a projector appearing as part of multiple measurements is that it must be assigned the same conditional probability $\{0,1\}$ by the ontological model, for all ontic states $\lambda$. In other words, operationally equivalence implies an identical ontological representation. We will find that Spekkens' revised notion of non-contextuality follows exactly this prescription.  

In \cite{Spekkens2005}, Spekkens introduces a largely operational notion of contextuality for preparations, measurements, and transformations. It is a revision of KS contextuality that addresses several shortcomings of the traditional definition. Contextuality is generalized to apply to arbitrary operational theories. In comparison, KS contextuality is limited to the scope of QM, as it is defined within the framework of quantum theory. Furthermore, the revised definiton will not assume outcome determinism at an ontic level, accomodating HVM whose indicator functions are more general mappings $\Lambda\mapsto [0,1]$. Taking QM as operational theory, Spekkens contextuality extends to arbitrary measurements, including physical, non-projective ones. In fact, within the Spekkens framework, measurements, preparations, and transformations can be considered black box devices, i.e.\ primitives of which one can compose an experiment. An operational theory makes predictions about these operational primitives, i.e.\ assigns probabilities to measurement outcomes that are consistent with experimental observations. In QM, for instance, outcome probabilities are given by the Born rule, where we posit that every preparation is operationally fully specified by a density operator and every measurement by a POVM. A Spekkens non-contextual operational theory is one that is compatible with a Spekkens non-contextual ontological model, which we will define shortly. Spekkens' revised concept of non-contextuality will serve as a new, more suitable notion of classicality that is in particular compatible with unsharp measurements.

This section only discusses Spekkens non-contextuality for preparations and measurements, as transformations will not be relevant to our discussion\footnote{Consider a prepare-and-measure type experiment that consists of an initial state preparation, followed by some transformation of the system, and finally a measurement. We may consider the transformation as part of the initial preparation procedure, or as part of the final measurement, and describe the experiment solely in terms of preparation procedures and measurements.}. Preparations and measurements will be represented as outlined in Section \ref{sec:hvm}. In the following, if not stated otherwise, the term ``contextuality" and all variant forms will refer to Spekkens contextuality.

The essence of what it means for an ontological model to be non-contextual can be summarized as follows:
An ontological model of an operational theory is \emph{non-contextual} if the operational equivalence of two experimental procedures, i.e.\ two preparations or two measurements, 
implies that they have an equivalent representation in the ontological model.
Two experimental procedures are operationally equivalent if they cannot be distinguished by any measurement statistics. 

Let us formalize the above, starting with preparation non-contextuality. We can define an equivalence relation on the set of all preparation procedures $P$ that partions the set into operational equivalence classes $[P]$. By the guiding principle above, we require equivalent preparations to have the same ontological representation in a non-contextual HVM.

\begin{definition}[\cite{Spekkens2005}]\hfill\break
\label{def:prepnc}
Two preparation procedures $P,P'$ are \emph{equivalent}, denoted $P\sim P'$, if 
\begin{equation*}
    \forall \text{ measurements } M,\thinspace \forall \text{ measurement outcomes } k\thinspace:\thinspace p(k\vert P,M)=p(k\vert P',M)\thinspace.
\end{equation*}

An ontological model is \emph{preparation non-contextual} if equivalent preparations have identical associated probability density functions on the ontic state space: 
\begin{equation*}
    \forall \text{ preparations } P\thinspace:\thinspace \mu_{P}=\mu_{[P]}\thinspace.
\end{equation*}
\end{definition}

Measurement non-contextuality is defined in an analogous manner:

\begin{definition}[\cite{Spekkens2005}]\hfill\break
\label{def:mntnc}
Two measurement procedures $M,M'$ are \emph{equivalent}, denoted $M\sim M'$, if their outcomes can be associated one-to-one and 
\begin{equation*}
    \forall \text{ outcomes } k\thinspace,\thinspace \forall \text{ preparations } P\thinspace:\thinspace p(k\vert P,M)=p(k\vert P,M')\thinspace.
\end{equation*}

An ontological model is \emph{measurement non-contextual} if equivalent measurements have identical associated indicator functions: 
\begin{equation*}
    \forall \text{ measurements } M\thinspace,\thinspace \forall \text{ outcomes } k\thinspace:\thinspace \xi_{M,k}=\xi_{[M],k}\thinspace.
\end{equation*}
\end{definition}

Spekkens proves three no-go theorems for ontological models of QM \cite{Spekkens2005}. These rule out preparation non-contextual, measurement non-contextual, and transformation non-contextual models, respectively. All three proofs apply to two-dimensional Hilbert spaces, making them stronger than traditional proofs of contextuality.

Whenever referring to a non-contextual ontological model, without specifying preparation or measurement contextual, we  will assume the ontological model to be both preparation and measurement non-contextual. As the reasons for assuming classical correlations to be compatible with a measurement non-contextual HVM description are the same as those for preparation non-contextuality, the combination of both is the only natural assumption of non-contextuality.

We now discuss to what extent non-contextuality is a sensible notion of classicality. A contextual ontological model implies a difference in reality that cannot be observed. Such a model is in conflict with Leibniz' ``Identity of Indiscernibles" \cite{Buchanan2019,Spekkens2019}. Leibniz's principle states that two empirically indistinguishable scenarios are to be ontologically equivalent. As such, it rejects ontological models for which there exist ontologically distinct, but empirically indistinguishable scenarios. Recall that, within QM, KS non-contextuality can be seen as a generalization of Bell's notion of local determinism to non-remote measurement contexts. One can also link Leibniz' principle, which is at the heart of Spekkens' revised notion of contextuality to Bell's notion of local causality. Like before, local causality is the weaker assumption, which is owed to the fact that Bell-type experiments impose additional constraints on the experiment's causal structure:

In principle, it might well be the case that an agent's free choice of measurement can have a causal influence on a remote system and alter the ontic or "matter of fact" state of that system. However, physicists have yet to devise an experiment producing correlations that are in conflict with the no-signalling principle, which is reinforced by special relativity. Therefore, to the best of our knowledge, such causal influence, if it exists, is not detectable. This in turn means that an agent in the remote frame of reference can detect no difference in the physical properties of his system for different free choices of measurement. Local causality is the assumption that there is in fact no difference at the ontic level, and thus provides the most natural explanation for our inability to detect a causal influence. Another, more hair-raising explanation could be that all possible ensemble preparations accessible to an experimenter are too spread out to resolve minor differences in the conditional probabilities. The assumption of local causality just corresponds to Leibniz' principle applied to Bell-type experiments with space-like separated measurements. The assumption of Spekkens non-contextuality simply extends the applicability of Leibniz's principle to arbitrary prepare-and-measure type experiments, in particular those without remote subsystems.

Just as local causality is motivated by the no-signalling principle, local causality providing the most natural explanation for our apparent inability to communicate superluminally, a point can be made that non-contextuality is perhaps equally well motivated by Leibniz' Identity of indiscernibles. Analogously to local causality, non-contextuality is the most natural explanation for operational equivalences. Spekkens points out that the credentials of Leibniz' principle parallel those of no-signalling and that it was critical in Einstein's conception of relativity \cite{Buchanan2019,Spekkens2019}. Take for instance the equivalence principle which states that one cannot distinguish between being at rest in a uniform gravitational field and accelerating uniformly through free space. Within Newtonian mechanics the two scenarios are ontologically distinct, as one's absolute acceleration differs in both cases. On his way to general relativity, Einstein reasoned that the empirical indistinguishablitiy of both scenarios implies that they should be treated as ontologically equivalent by a physical theory, a powerful invocation of Leibniz's principle that reshaped our perception of reality. An almost identical line of reasoning is found in Einsteinâ€™s 1905 paper ``On the electrodynamics of moving bodies" \cite{Einstein1905}. He notes that the predictions of Maxwell's equations depend only on relative motion. For example, the current induced in a coil will have the same magnitude and direction, no matter if we consider a magnet to be moving through the coil or the coil moving through the field of the magnet with equal but opposite velocity. To Einstein, this empirical indistinguishability was in conflict with the prevailing aether theories of the time. An aether would define a distinguished frame of reference, rendering the two empirically equivalent induction experiments ontologically distinct. Once again, Einstein's invocation of Lebniz' principle lead him to abandon aether theory and conceive relativity theory, in which both scenarios are equivalent.

One can lift KS non-contextuality inequalities to Spekkens non-contextuality inequalities by substituting assumptions about operator compatibility and sharpness for assumptions about operational equivalences \cite{Kunjwal2019}. Like before, these lifted non-contextuality inequalities, when violated, bear witness to the quantumness of the system, in the Spekkens sense. Violations of non-contextuality inequalities that prove nature's incompatibility with a non-contextual HVM description have been observed in experiments involving photonic qubit systems \cite{Mazurek2016}. While the lifted non-contextuality inequalities no longer make unphysical assumptions about the measurements themselves, they introduce two new practical complications: the issue of tomographic completeness, and that of exact operational equivalence. Experimental tests of non-contextuality inequalities assume that we can implement a tomographically complete (TC) set of measurements, relative to the accessible\footnote{An ``accessible" preparation procedure is one that is implemented by the experimental apparatus at hand, and whose statistics are recorded as part of the experiment.} preparations, and a TC set of preparations, relative to the accessible measurements. The reason for this is that, according to Definitons \ref{def:prepnc}, \ref{def:mntnc}, two experimental procedures can only be asserted operationally equivalent if the two procedures are operationally indistinguishable, for all possible prepare-and-measure-type experiment that utilize them. For instance, two preparations are only operationally equivalent if they produce the same statistics for all possible measurements, requiring in principle an infinite number of tests. In practice, we aim to identify a TC set of measurements with respect to the two preparations, as defined in REF, such that there is a functional relationship between the statistics of an arbitrary measurement performed on one of the available preparations and the statistics of only the measurements of the TC set. As such, if two preparations are operationally equivalent with respect to a TC set of measurements, they are operationally equivalent with respect to all measurements. Within QM, a TC set of measurements for a qubit system is given by the three Pauli measurements $\sigma_x$, $\sigma_y$, and $\sigma_z$, that together determine the three Bloch vector components of an arbitrary state.

\begin{definition}[\cite{Pusey2019a}]
\label{def:tc}
A set of binary measurements (with outcomes 0,1) $\mathcal{M}_C$ is tomographically complete (TC), relative to a set of preparations $\mathcal{P}$, if for any measurements $M$ (not necessarily contained in $\mathcal{M}$) with outcome set $\mathcal{K}_{M}$ there exists a deterministic function $f_M:\mathcal{K}_{M}\times[0,1]^{\vert\mathcal{M}_C \vert}\rightarrow [0,1]$ with the properties that
\begin{enumerate}
\item $\forall k\in\mathcal{K}_{M}:\;$ $f(k\,,\,\cdot):[0,1]^{\vert\mathcal{M}_C\vert}\rightarrow [0,1]$ is a linear mapping, and
\item $\forall k\in\mathcal{K}_{M}\,\forall P\in\mathcal{P}$: $p(k\,\vert\, P,M)=f_M(k,\{p(0\,\vert\,P,M')\}_{M'\in\mathcal{M}_C})$,
\end{enumerate}
where $p(k\,\vert\, P,M)$ is the empirical probability of obtaining the outcome $k$ when performing the measurement $M$ on a system prepared like $P$.
\end{definition}
Note that Condition 1 ensures that the mapping $f_M$ is compatible with convex mixtures of preparation procedures.

Problematically, without additional assumptions, we can in practice never declare a set of measurements to be TC with respect to some set of preparations. What we can do is try our best to refute such a claim. Additionally, there are tests of contextuality that account for some number of unknown degrees of freedom, in particular some number of unknown measurements that complete the accessible measurements to a TC set \cite{Pusey2019a}. These strengthened tests of contextuality require knowledge about an upper bound on the information carrying capacity of the system, i.e. the amount of information (in bits) that one can encode in the state of the system in a retrievable manner. By implementing more preparations and measurements one can in principle account for an arbitrary number of unknown degrees of freedom, although the trade-off is exponential. Section \ref{sec:protocols} will make use of the results in \cite{Pusey2019a}, which will ultimately allow us to certify the quantumness of the unknown device in a robust manner, assuming an upper bound on its information carrying capacity. Just like in the case of KS contextuality, where one had to assume operator compatibility and sharp measurements, we cannot expect to devise a fully device-independent self-testing protocol based on non-contextuality.

Another issue with experimental tests of contextuality is that Definitions \ref{def:prepnc}, \ref{def:mntnc} assume exact operational equivalences, requiring infinite precision. This problem has been tackled in \cite{Pusey2018,Mazurek2016}. Let us for the moment consider operational equivalences amongst preparations. The key is noticing that performing measurements on some set of preparations defines the statistics for all preparations in the convex hull. As part of post-processing, we can define new preparations in the convex hull, whose statistics we know, and that are exactly operationally equivalent. The price to pay is that these new preparations may be less optimal than the original ones. This ``fitting" process that establishes exact operational equivalences is done within the framework of generalized probabilistic theories (GPT). Operational equivalences amongst measurements are treated in an analogous manner. The protocol we propose in Section \ref{sec:protocols} in fact only assumes approximate operational equivalence of certain preparations.

Finally, we wish to understand how KS contextuality and Spekkens contextuality are related. It turns out that a Spekkens non-contextual ontological model of QM assign deterministic outcomes to projective quantum measurements and induces a KS non-contextual outcome assignment, like in Definition \ref{def:kscontherm}.
KS non-contextuality is an assumption that restricts the ontological representation of compatible (commuting) projective measurements. Let us therefore examine what conditions Spekkens non-contextuality imposes on the ontological representation of compatible measurements. Spekkens non-contextuality covers arbitrary, not necessarily sharp, measurements and is applicable to arbitrary operational theories. Therefore, recall Definition \ref{def:compat}, which defines what it means for two measurements to be compatible, purely in operational terms.

Let $M_{1}$ and $M_{2}$ be compatible measurements according to Definition \ref{def:compat}, and $M_{12}$ a dual outcome measurement that jointly realizes $M_1$ and $M_2$. By definition, the measurement procedure $M_{1}$ is operationally equivalent to measuring $M_{12}$ and discarding register two. Analogously, the measurement procedure $M_{2}$ is operationally equivalent to measuring $M_{12}$ and discarding register one. What can be said about the ontological representation of compatible measurements within a Spekkens non-contextual model? In a Spekkens non-contextual ontological model, operationally equivalent measurement procedures have identical associated indicator functions. For the measurements $M_{1}$, $M_{2}$, and $M_{12}$ like above, this implies:
\begin{align*}
\forall\thinspace\lambda\in\Lambda,\thinspace\forall \thinspace P,\thinspace\forall\thinspace a_{k}:p(a_{k}\thinspace\vert\thinspace M_{1},P,\lambda)=\sum_{b_{j}}p(a_{k},b_{j}\thinspace\vert\thinspace M_{12},P,\lambda)\\
\forall\thinspace\lambda\in\Lambda,\forall\thinspace P,\thinspace\forall\thinspace b_{k}:p(b_{k}\thinspace\vert\thinspace M_{2},P,\lambda)=\sum_{a_{i}}p(a_{i},b_{k}\thinspace\vert\thinspace M_{12},P,\lambda)
\end{align*}
Consequently, a Spekkens measurement non-contextual ontological model is one for which the probability of measuring say $M_{1}$ and obtaining the outcome $a_{k}$, conditioned on the system being in any ontic state, is independent of what other compatible measurements are simultaneously performed. If we consider QM as operational theory, this is the essence of KS non-contextuality, as discussed in Section \ref{sec:kscont}, with the notable difference that we have decoupled outcome determinism from context independence.

Interestingly, it can proven that a Spekkens non-contextual ontological model must fix the outcomes of sharp measurements deterministically \cite{Spekkens2014}. This result is robust, in the sense that ``almost sharp" measurements are assigned outcomes ``almost deterministically". Thus, for projective measurements, the notion of Spekkens non-contextuality reduces to KS non-contextuality. For Spekkens non-contextual ontological models, outcome determinism for sharp measurements can thus be derived from within the framework itself, and is not an assumption introduced ad-hoc, as was the case for KS non-contextuality.